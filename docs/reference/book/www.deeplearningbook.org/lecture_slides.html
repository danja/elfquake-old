<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0044)http://www.iro.umontreal.ca/~bengioy/dlbook/ -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
 
    <title>Deep Learning</title>
  </head>
  <body>
    <style type="text/css">
      html{
        background-image: url("udem.jpg");
        background-position:center top;
        background-repeat:no-repeat;
        width:800px;
        margin:auto;
    }
    ol {

      list-style: none;
    }

    ol > li:before {
      content: attr(seq) ". ";
    }
    </style>
    <h1><a href="front_matter.pdf">Deep Learning</a></h1>
    <h2>An MIT Press book in preparation</h2>
    <h3>Ian Goodfellow, Yoshua Bengio and Aaron Courville</h3>
    <br>
<div>
    <center>
    <a href="index.html">Book</a> &nbsp
    <a href="exercises.html">Exercises</a>
    <a href="external.html">External Links</a> &nbsp
</center>
</div>
<h2>Lectures</h2>
<p>We plan to offer lecture slides accompanying all chapters of this book.
We currently offer slides for only some chapters.
If you are a course instructor and have your own lecture slides that are
relevant, feel free to contact us if you would like to have your slides
linked or mirrored from this site.</p>

<ol>
 <li seq="1"> <a href="contents/intro.html">Introduction</a> [<a href="slides/01_intro.key">.key</a>]
              [<a href="slides/01_intro.pdf">.pdf</a>]
 </li>
 <li seq="2"> <a href="contents/linear_algebra.html">Linear Algebra</a>
  [<a href="slides/02_linear_algebra.key">.key</a>][<a href="slides/02_linear_algebra.pdf">.pdf</a>]
 </li>
 <li seq="3"> <a href="contents/prob.html">Probability and Information Theory</a>
  [<a href="slides/03_prob.key">.key</a>][<a href="slides/03_prob.pdf">.pdf</a>]
</li>
<li seq="4"> <a href="contents/numerical.html">Numerical Computation</a>
  [<a href="slides/04_numerical.key">.key</a>] [<a href="slides/04_numerical.pdf">.pdf</a>]
</li>
<li seq="5"> <a href="contents/ml.html">Machine Learning Basics</a>
  [<a href="slides/05_ml.key">.key</a>] [<a href="slides/05_ml.pdf">.pdf</a>]
</li>
<li seq="6"> <a href="contents/mlp.html">Deep Feedforward Networks</a>
[<a href="slides/06_mlp.key">.key</a>] [<a href="slides/06_mlp.pdf">.pdf</a>]
</li>
<li seq="7"><a href="contents/regularization.html">Regularization for Deep Learning</a>
  [<a href="slides/07_regularization.pdf">.pdf</a>] [<a href="slides/07_regularization.key">.key</a>]
</li>
 <li seq="8"> <a href="contents/optimization.html">Optimization for Training Deep Models</a>
 <ul>
  <li><b>Gradient Descent and Structure of Neural Network Cost Functions</b> [<a href="slides/sgd_and_cost_structure.key">.key</a>]
  [<a href="slides/sgd_and_cost_structure.pdf">.pdf</a>] <br>
  These slides describe how gradient descent behaves on different kinds of cost function
  surfaces.
  Intuition for the structure of the cost function can be built by examining a second-order
  Taylor series approximation of the cost function.
  This quadratic function can give rise to issues such as poor conditioning and saddle points.
  Visualization of neural network cost functions shows how these and some other geometric
  features of neural network cost functions affect the performance of gradient descent.
  </li>
  <li><b>Tutorial on Optimization for Deep Networks</b> [<a href="slides/dls_2016.key">.key</a>] [<a href="slides/dls_2016.pdf">.pdf</a>]<br>
  Ian's presentation at the 2016 Re-Work Deep Learning Summit. Covers Google Brain research on optimization,
  including visualization of neural network cost functions, Net2Net, and batch normalization.
  </li>
  <li><b>Batch Normalization</b> [<a href="slides/batch_norm.key">.key</a>] [<a href="slides/batch_norm.pdf">.pdf</a>]</li>
   <li><b><a href="https://www.youtube.com/watch?v=Xogn6veSyxA">Video</a> of lecture / discussion</b>:
    This video covers a presentation by Ian and group discussion on the end of Chapter 8 and entirety of Chapter 9
    at a reading group in San Francisco organized by Taro-Shigenori Chiba.
   </li>
</ul></li>
  <li seq="9"> <a href="contents/convnets.html">Convolutional Networks</a>
  <ul>
   <li><b>Convolutional Networks</b> [<a href="slides/09_conv.key">.key</a>][<a href="slides/09_conv.pdf">.pdf</a>]<br>
     A presentation summarizing Chapter 9, based directly on the textbook itself.
     </li>
   <li><b><a href="https://www.youtube.com/watch?v=Xogn6veSyxA">Video</a> of lecture / discussion</b>:
    This video covers a presentation by Ian and group discussion on the end of Chapter 8 and entirety of Chapter 9
    at a reading group in San Francisco organized by Taro-Shigenori Chiba.
   </li>
  </ul>
  </li>
<li seq="10"><a href="contents/rnn.html">Sequence Modeling: Recurrent and Recursive Networks</a>
  [<a href="slides/10_rnn.pdf">.pdf</a>] [<a href="slides/10_rnn.key">.key</a>]
</li>
 <li seq="11"> <a href="contents/guidelines.html">Practical Methodology</a>
  [<a href="slides/11_practical.key">.key</a>][<a href="slides/11_practical.pdf">.pdf</a>]
  [<a href="https://www.youtube.com/watch?v=ccyClyHAIdI">youtube</a>]
 </li>
 <li seq="12"> <a href="contents/applications.html">Applications</a>
  [<a href="slides/12_applications.key">.key</a>][<a href="slides/12_applications.pdf">.pdf</a>]
 </li>
 <li seq="13"> <a href="contents/linear_factors.html">Linear Factors</a>
 [<a href="slides/13_linear_factors.key">.key</a>][<a href="slides/13_linear_factors.pdf">.pdf</a>]
 </li>
 <li seq="14"> <a href="contents/autoencoders.html">Autoencoders</a>
 [<a href="slides/14_autoencoders.key">.key</a>][<a href="slides/14_autoencoders.pdf">.pdf</a>]
 </li>
</ol>
  </body>
</html>
</ul></body></html>
