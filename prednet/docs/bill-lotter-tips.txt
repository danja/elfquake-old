Thanks for your interest in our work and it seems like a cool application.  Like you mention, it seems like the tricky part will be accounting for the depth.  I imagine you could do several things:  1) only predict for a certain depth, 2) take a max/average over the depth at each time point, 3) train on all the depths, that is for each training sequence, pick a random depth (I'm not sure if this makes sense from a physics standpoint), 4) treat the depth as a "channel", i.e. like a color channel.

As far as processing the data, how much data overall will it be?  Using HDF5 files aren't strictly necessary, you just need a numpy array at some point that would have dimensions (batch size, # timesteps, # channels - 1 if using one particular depth, or N for N depths, lat, long).  But overall, it might be easiest to process the data from csv into an HDF5 for storing the data.  That would just look something like this:

import numpy as np
import hickle as hkl

X = np.zeros((nt, n_depth, n_lat, n_long), np.float32)

# read csv file and fill in X

hkl.dump(X, 'data.hkl')
